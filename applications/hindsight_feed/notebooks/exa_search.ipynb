{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exa_py import Exa\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from config import EXA_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exa = Exa(api_key=EXA_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    \"AI and Machine Learning\": \"Development and applications of AI technologies like MultiOn AI and mlx-image for Apple Silicon machines.\",\n",
    "    \"Software Development Tools and APIs\": \"Information about building and using APIs such as Exa API, MultiOn AI Agent API, and ChatRTX by NVIDIA.\",\n",
    "    \"Productivity and Time Management\": \"Tools like RescueTime for tracking and managing time.\",\n",
    "    \"News and Content Aggregation\": \"Discussions and tools for creating personalized news feeds and content aggregation, including references to Reddit, Stream Personalization, and RSS feeds.\",\n",
    "    \"Open Source Projects and Community\": \"Information about specific GitHub projects and tools for software development and content management.\",\n",
    "    \"Web Browsers and History Management\": \"Techniques and tools related to managing and exporting browser history.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_exa_results = {}\n",
    "for topic, description in topics.items():\n",
    "    topic_description = f\"{topic}:{description}\"\n",
    "    exa_result = exa.search_and_contents(\n",
    "        topic_description,\n",
    "        type=\"neural\",\n",
    "        use_autoprompt=True,\n",
    "        num_results=25,\n",
    "        text=True,\n",
    "        highlights=True,\n",
    "        start_published_date=\"2024-07-01\"\n",
    "        )\n",
    "    topic_to_exa_results[topic] = exa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_l = list()\n",
    "for topic, exa_result in topic_to_exa_results.items():\n",
    "    for res in exa_result.results:\n",
    "        res_d = res.__dict__\n",
    "        res_d['topic'] = topic\n",
    "        results_l.append(res_d)\n",
    "results_df = pd.DataFrame(results_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://webkit.org/blog/15697/private-browsing-2-0/',\n",
       " 'id': 'https://webkit.org/blog/15697/private-browsing-2-0/',\n",
       " 'title': 'Private Browsing 2.0',\n",
       " 'score': 0.11516977846622467,\n",
       " 'published_date': '2024-07-16T00:00:00.000Z',\n",
       " 'author': 'John Wilander; Charlie Wolfe; Matthew Finkel; Wenson Hsieh; Keith Holleman',\n",
       " 'text': 'When we invented Private Browsing back in 2005, our aim was to provide users with an easy way to keep their browsing private from anyone who shared the same device. We created a mode where users do not leave any local, persistent traces of their browsing. Eventually all other browsers shipped the same feature. At times, this is called “ephemeral browsing.”\\nWe baked in cross-site tracking prevention in all Safari browsing through our cookie policy, starting with Safari 1.0 in 2003. And we’ve increased privacy protections incrementally over the last 20 years. (Learn more by reading Tracking Prevention in Webkit.) Other popular browsers have not been as quick to follow our lead in tracking prevention but there is progress.\\nApple believes that users should not be tracked across the web without their knowledge or their consent. Entering Private Browsing is a strong signal that the user wants the best possible protection against privacy invasions, while still being able to enjoy and utilize the web. Staying with the 2005 definition of private mode as only being ephemeral, such as Chrome’s Incognito Mode, simply doesn’t cut it anymore. Users expect and deserve more.\\nSo, we decided to take Private Browsing further and add even more protection beyond the normal Safari experience. Last September, we added a whole new level of privacy protections to Private Browsing in Safari 17.0. And we enhanced it even further in Safari 17.2 and Safari 17.5. Plus, when a user enables them, all of the new safeguards are available in regular Safari browsing too.\\nWith this work we’ve enhanced web privacy immensely and hope to set a new industry standard for what Private Browsing should be.\\nEnhanced Private Browsing in a Nutshell\\nThese are the protections and defenses added to Private Browsing in Safari 17.0:\\nLink Tracking Protection\\nBlocking network loads of known trackers, including CNAME-cloaked known trackers\\nAdvanced Fingerprinting Protection\\nExtensions with website or history access are off by default\\nIn addition, we added these protections and defenses in all browsing modes:\\nCapped lifetime of cookies set in responses from cloaked third-party IP addresses\\nPartitioned SessionStorage\\nPartitioned blob URLs (starting in Safari 17.2)\\nWe also expanded Web AdAttributionKit (formerly Private Click Measurement) as a replacement for tracking parameters in URL to help developers understand the performance of their marketing campaigns even under Private Browsing.\\n Private Browsing in Safari \\nHowever, before we dive into these new and enhanced privacy protections, let’s first consider an important aspect of these changes: website compatibility risk.\\nThe Risk of Breaking Websites and How We Mitigate It\\nThere are many ideas for how to protect privacy on the web, but unfortunately many of them may break the user’s experience. Like security protections in real life, a balance must be struck. The new Private Browsing goes right up to the line, attempting to never break websites. But of course there is a risk that some parts of some sites won’t work. To solve this, we give users affordances to reduce privacy protections on a per-site basis. Such a change in privacy protections is only remembered while browsing within a site. This option is a last resort when a web page is not usable due to the privacy protections.\\n Reload Reducing Privacy Protections \\nAll of the new privacy protections in Private Browsing are also available in regular browsing. On iOS, iPadOS and visionOS go to Settings &gt; Apps &gt; Safari &gt; Advanced &gt; Advanced Tracking and Fingerprinting Protection and enable “All Browsing”. On macOS go to Safari &gt; Settings &gt; Advanced and enable “Use advanced tracking and fingerprinting protection”:\\n Use advanced tracking and fingerprinting protection in all browsing from Safari Advanced Settings \\nLet’s now walk through how these enhancements work.\\nLink Tracking Protection\\nSafari’s Private Browsing implements two new protections against tracking information in the destination URL when the user navigates between different websites. The specific parts of the URL covered are query parameters and the fragment. The goal of these protections is to make it more difficult for third-party scripts running on the destination site to correlate user activity across websites by reading the URL.\\nLet’s consider an example where the user clicks a link on clickSource.example, which takes them to clickDestination.example. The URL looks like this:\\n https://clickDestination.example/article?known_tracking_param=123&amp;campaign=abc&amp;click_val=456\\n \\nSafari removes a subset of query parameters that have been identified as being used for pervasive cross-site tracking granular to users or clicks. This is done prior to navigation, such that these values are never propagated over the network. If known_tracking_param above represents such a query parameter, the URL that’s used for navigation will be:\\n https://clickDestination.example/article?campaign=abc&amp;click_val=456\\n \\nAs its name suggests, the campaign above represents a parameter that’s only used for campaign attribution, as opposed to click or user-level tracking. Safari allows such parameters to pass through.\\nFinally, on the destination site after a cross-site navigation, all third-party scripts that attempt to read the full URL (e.g. using location.search, location.href, or document.URL) will get a version of the URL that has no query parameters or fragment. In our example, this script-exposed value is simply:\\n https://clickDestination.example/article\\n \\nIn a similar vein, Safari also hides cross-site any document.referrer from script access in Private Browsing.\\nWeb AdAttributionKit in Private Browsing\\nWeb AdAttributionKit (formerly Private Click Measurement) is a way for advertisers, websites, and apps to implement ad attribution and click measurement in a privacy-preserving way. You can read more about it here. Alongside the new suite of enhanced privacy protections in Private Browsing, Safari also brings a version of Web AdAttributionKit to Private Browsing. This allows click measurement and attribution to continue working in a privacy-preserving manner.\\nWeb AdAttributionKit in Private Browsing works the same way as it does in normal browsing, but with some limits:\\nAttribution is scoped to individual Private Browsing tabs, and transfers attribution across new tabs opened when clicking on links. However, attribution is not preserved through other indirect means of navigation: for instance, copying a link and pasting in a new tab. In effect, this behaves similarly to how Web AdAttributionKit works for Direct Response Advertising.\\nSince Private Browsing doesn’t persist any data, pending attribution requests are discarded when the tab is closed.\\nBlocking Network Loads of Known Trackers\\nSafari 17.0 also comes with an automatically enabled content blocker in Private Browsing, which blocks network loads to known trackers. While Intelligent Tracking Prevention has long blocked all third party cookies, blocking trackers’ network requests from leaving the user’s device in the first place ensures that no personal information or tracking parameters are exfiltrated through the URL itself.\\nThis automatically enabled content blocker is compiled using data from DuckDuckGo and from the EasyPrivacy filtering rules from EasyList. The requests flagged by this content blocker are only entries that are flagged as trackers by both DuckDuckGo and EasyPrivacy. In doing so, Safari intentionally allows most ads to continue loading even in Private Browsing.\\nPrivate Browsing also blocks cloaked network requests to known tracking domains. They otherwise have the ability to save third party cookies in a first-party context. This protection requires macOS Sonoma or iOS 17. By cloaked we mean subdomains mapped to a third-party server via CNAME cloaking or third-party IP address cloaking. See also the “Defending Against Cloaked First Party IP Addresses” section below.\\nWhen Safari blocks a network request to a known tracker, a console message of this form is logged, and can be viewed using Web Inspector:\\n ` .`\\n \\nNetwork Privacy Enhancements\\nSafari 15.0 started hiding IP addresses from known trackers by default. Private Browsing in Safari 17.0 adds the following protections for all users:\\n Encrypted DNS. DNS queries are used to resolve server hostnames into IP addresses, which is a necessary function of accessing the internet. However, DNS is traditionally unencrypted, and allows network operators to track user activity or redirect users to other servers. Private Browsing uses Oblivious DNS over HTTPS by default, which encrypts and proxies DNS queries to protect the privacy and integrity of these lookups.',\n",
       " 'highlights': ['The Risk of Breaking Websites and How We Mitigate It There are many ideas for how to protect privacy on the web, but unfortunately many of them may break the user’s experience. Like security protections in real life, a balance must be struck. The new Private Browsing goes right up to the line, attempting to never break websites. But of course there is a risk that some parts of some sites won’t work.'],\n",
       " 'highlight_scores': [0.2985684871673584],\n",
       " 'summary': None,\n",
       " 'topic': 'Web Browsers and History Management'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction:\\nWelcome to an exciting series on optimizing cutting-edge AI models for Apple Silicon! Over the next few weeks, we\\'ll dive deep into the process of porting Phi-3-Vision, a powerful and compact vision-language model, from Hugging Face to MLX.\\nThis series is designed for AI enthusiasts, developers, and researchers interested in running advanced models efficiently on Mac devices. For those eager to get started, you can find the MLX ports of both Phi-3-Mini-128K and Phi-3-Vision in my GitHub repository: https://github.com/JosefAlbers/Phi-3-Vision-MLX.\\nWhy Phi-3-Vision?\\nWhen Microsoft Research released Phi-3, I was immediately intrigued. Despite its relatively small size of 3.8 billion parameters, it was performing on par with or even surpassing models with 7 billion parameters. This efficiency was impressive and hinted at the potential for running sophisticated AI models on consumer-grade hardware.\\n   \\nThe subsequent release of Phi-3-Vision further piqued my interest. As an owner of a Mac Studio and a Python hobbyist, I saw an exciting opportunity to bring this capable vision-language model to Apple Silicon. While llama.cpp was a popular option for running large language models on Mac, its C++ codebase was way beyond my skill level, so I looked for a more accessible option. This led me to MLX, Apple\\'s machine learning framework that not only offered a Python-friendly environment but also promised better performance than llama.cpp on Apple Silicon.\\n   \\nWhat made this journey even more exciting was that it marked my first foray into contributing to open source projects. As I worked on porting Phi-3-Vision, I found myself making my first pull requests to repositories like \"mlx-examples\" and \"mlx-vlm\". This experience was an invaluable learning experience that helped me gain a better understanding of the MLX framework and how to apply it to real-world projects. This experience also connected me with the broader AI development community.\\nUseful Resources:\\nBefore we dive into the series, I want to highlight some excellent resources that have been invaluable in my journey:\\nMLX Examples (https://github.com/ml-explore/mlx-examples): This official repository from the MLX team at Apple is a treasure trove of examples and tutorials that showcase the capabilities of the MLX framework. With a wide range of standalone examples, from basic MNIST to advanced language models and image generation, this repository is an excellent starting point for anyone looking to learn MLX. The quality and depth of the examples are truly impressive, and they demonstrate the team\\'s commitment to making MLX accessible to developers of all levels.\\n   \\nI also want to give a special shoutout to awni, the repo owner, who was incredibly kind and patient with me when I made my first-ever pull request to this repository. Despite my lack of experience with Git and GitHub, awni guided me through the process and helped me navigate the precommit hooks and other nuances of the repository. Their patience and willingness to help a newcomer like me was truly appreciated, and I\\'m grateful for the opportunity to have contributed to this repository. If you\\'re new to MLX or Git, I highly recommend checking out this repository and reaching out to awni - they\\'re a great resource and a pleasure to work with!\\nMLX-VLM (https://github.com/Blaizzy/mlx-vlm): A package specifically for running Vision Language Models on Mac using MLX. This repository was particularly helpful in understanding how to handle multimodal inputs, and I found the well-organized and well-written code to be incredibly valuable in learning not only Vision Language Models (VLMs) but also the MLX framework in general. The codebase is a great example of how to structure and implement complex AI models using MLX, making it an excellent resource for anyone looking to learn from experienced developers and improve their own MLX skills. For those interested in other models, Prince Canuma has an excellent tutorial on running Google\\'s Gemma 2 locally on Mac using MLX:\\n Hugging Face (https://huggingface.co/): A popular platform for natural language processing (NLP) and computer vision tasks, providing a vast range of pre-trained models, datasets, and tools. Hugging Face’s Transformers library is particularly useful for working with transformer-based models like Phi-3-Vision. Their documentation and community support are also top-notch, making it an excellent resource for anyone looking to learn more about NLP and computer vision. \\nThese resources provide a great foundation for anyone looking to explore MLX and run advanced AI models on Apple Silicon.\\nWhat to Expect in This Series:\\n1. MLX vs. Hugging Face: A Code Comparison\\nWe\\'ll start by comparing the original Hugging Face implementation with our MLX port, highlighting key differences in syntax and how MLX leverages Apple Silicon\\'s unified memory architecture.\\n2. Implementing Su-RoPE for 128K Context\\nWe\\'ll explore the Surrogate Rotary Position Embedding (Su-RoPE) implementation that enables Phi-3-Vision to handle impressive 128K token contexts.\\n3. Optimizing Text Generation in MLX: From Batching to Advanced Techniques\\nLearn how to implement efficient batch text generation, a crucial feature for many real-world applications. We\\'ll also cover custom KV-Cache implementation, streaming capabilities, and other text generation optimizations.\\n4. LoRA Fine-tuning and Evaluation on MLX\\nDiscover how to perform Low-Rank Adaptation (LoRA) training, enabling efficient fine-tuning of Phi-3-Vision on custom datasets. We\\'ll also develop comprehensive evaluation techniques to ensure our LoRA-adapted model meets or exceeds the original\\'s performance.\\n5. Building a Versatile AI Agent\\nIn our finale, we\\'ll create a multi-modal AI agent showcasing Phi-3-Vision\\'s capabilities in handling both text and visual inputs.\\nWhy This Series Matters:\\nPhi-3-Vision represents a significant advancement in compact, high-performing vision-language models. By porting it to MLX, we\\'re making it more accessible and efficient for a wide range of applications on Apple Silicon devices. This project demonstrates the potential of running advanced AI models on consumer-grade hardware, specifically Apple Silicon Macs.\\nWho This Series Is For:\\nAI enthusiasts and hobbyists looking to dive deeper into model optimization\\nResearchers exploring efficient AI on consumer hardware\\nMac users eager to leverage their devices for AI tasks\\nAnyone curious about the intersection of AI and Apple Silicon\\nBeginners interested in contributing to open source AI projects\\nStay Tuned!\\nOur journey into optimizing Phi-3-Vision for MLX promises to be full of insights, challenges, and exciting breakthroughs. Whether you\\'re a fellow hobbyist looking to run advanced AI models on your Mac or simply curious about the future of AI on consumer devices, this series has something for you.\\nJoin me on this adventure in AI optimization, and let\\'s unlock the full potential of Phi-3-Vision on Apple Silicon together!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Whether you're a fellow hobbyist looking to run advanced AI models on your Mac or simply curious about the future of AI on consumer devices, this series has something for you. Join me on this adventure in AI optimization, and let's unlock the full potential of Phi-3-Vision on Apple Silicon together!\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.iloc[0]['highlights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'id', 'title', 'score', 'published_date', 'author', 'text',\n",
       "       'highlights', 'highlight_scores', 'summary', 'topic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"exa_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add articles to personal feed database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"exa_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df['thumbnail_url'] = results_df['url'].apply(lambda x: utils.get_thumbnail_url(x))\n",
    "# results_df['ranking_score'] = results_df['score']\n",
    "# results_df['title'] = results_df['title'].fillna(results_df['text']) # For tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.df_add_articles(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arts = articles.fetch_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Article(title=Using Local Browser Storage in .NET MAUI Blazor Hybrid, url=https://dev.to/nick_alonge/using-local-browser-storage-in-net-maui-blazor-hybrid-3loe, published_date=2024-07-18, score=None, clicked=False)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arts[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles.fetch_articles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"ai_ml_exa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'author',\n",
       " 'highlight_scores',\n",
       " 'highlights',\n",
       " 'id',\n",
       " 'published_date',\n",
       " 'score',\n",
       " 'summary',\n",
       " 'text',\n",
       " 'title',\n",
       " 'url']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(result.results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://levelup.gitconnected.com/4m-21-multitasking-multimodal-vision-model-by-apple-56268d0ea64f?gi=3656b2247a26&source=rss----5517fd7b58a6---4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['url'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = utils.get_thumbnail_url('https://levelup.gitconnected.com/4m-21-multitasking-multimodal-vision-model-by-apple-56268d0ea64f?gi=3656b2247a26&source=rss----5517fd7b58a6---4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://miro.medium.com/v2/resize:fit:1200/1*dEE6uW80Lhy0HlJ3EWyFMQ.png'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hindsight_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
