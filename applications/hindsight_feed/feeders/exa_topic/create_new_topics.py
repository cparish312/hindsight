import pandas as pd

from bertopic import BERTopic

from bertopic.representation import KeyBERTInspired
from bertopic.representation import PartOfSpeech
from bertopic.representation import MaximalMarginalRelevance

from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN

from hindsight_feed_db import fetch_content_generators, fetch_contents

import sys
sys.path.insert(0, "../../../../")
sys.path.insert(0, "../../../")
sys.path.insert(0, "./")

from hindsight_server.config import LLM_MODEL_NAME
from hindsight_server.query.query import load, llm_generate

def content_to_df(content):
    content_list = list()
    for c in content:
        d = c.__dict__
        d.update(c.content_generator_specific_data)
        content_list.append(d)
    return pd.DataFrame(content_list)

def get_topic_model():
    embedding_model = SentenceTransformer('all-mpnet-base-v2')
    umap_model = UMAP(n_neighbors=20, n_components=3, min_dist=0.005)
    hdbscan_model = HDBSCAN(min_cluster_size=5, min_samples=5,
                            gen_min_span_tree=True,
                            prediction_data=True)

    main_representation = KeyBERTInspired()

    # Additional ways of representing a topic
    aspect_model1 = PartOfSpeech("en_core_web_sm")
    aspect_model2 = [KeyBERTInspired(top_n_words=30), MaximalMarginalRelevance(diversity=.5)]

    # Add all models together to be run in a single `fit`
    representation_model = {
    "Main": main_representation,
    "Aspect1":  aspect_model1,
    "Aspect2":  aspect_model2 
    }
    topic_model = BERTopic(umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                embedding_model=embedding_model,
                representation_model=representation_model)
    return topic_model

def get_dive_deeper_prompt(content_df):
    prompt = """You are an assistant who generates search queries for an embedding search database to help a user find content of interest.

    Below are excerpts from content that the user has recently engaged with:

    """
    for i, row in content_df.iterrows():
        prompt += f"Title: {row['title']}\n"
        content_text = row['text']
        prompt += f"{content_text}\n\n"

    prompt += """Task:
        - Analyze the content above.
        - Identify the main themes and topics.
        - Generate a single, concise sentence that summarizes the user's interests.
        - Include as many relevant keywords and concepts from the content as possible.
        - The sentence should be suitable as a search query for finding similar content.

        Answer:"""
    return prompt

def refine_search_query_prompt(initial_response):
    prompt = """You are an assistant tasked with refining a search query generated by a previous analysis of user interests. Below is the initial query generated:

    '{initial_response}'

    Task:
    - Review the initial query.
    - Clarify and condense the query to better capture the essential themes and keywords.
    - Ensure the refined query is sharp, specific, and optimized for searching related content.
    - The refined query should be a single, succinct sentence that effectively encapsulates the user's core interests.

    Refined Query:"""
    return prompt.format(initial_response=initial_response)

def create_topic_from_previous_topic(topic_df, llm_pipeline):
    prompt = get_dive_deeper_prompt(topic_df)
    response = llm_generate(pipeline=llm_pipeline, prompt=prompt, max_tokens=80)
    refine_prompt = refine_search_query_prompt(response)
    refine_response = llm_generate(pipeline=llm_pipeline, prompt=refine_prompt, max_tokens=80)
    return refine_response.split("\n")[1] # Not sure how generalizable this will be

def create_new_topics(num_topics=5):
    content_generators = fetch_content_generators()

    exa_content_generators = [cg for cg in content_generators if cg.gen_type == "ExaTopicFeeder"]
    exa_content_generators_ids = {cg.id for cg in exa_content_generators}
    exa_content_generators_topics = {cg.parameters['topic'] for cg in exa_content_generators}

    content = fetch_contents(non_viewed=False)

    exa_content = [c for c in content if c.content_generator_id in exa_content_generators_ids]
    exa_content_df = content_to_df(exa_content)
    
    # Only get viewed content
    exa_content_df = exa_content_df.loc[exa_content_df['viewed']]

    topic_model = get_topic_model()

    topics, probs = topic_model.fit_transform(exa_content_df['text'])

    doc_info = topic_model.get_document_info(exa_content_df['text'])

    content_with_topics = doc_info.merge(exa_content_df, left_on="Document", right_on='text')
    
    # Calculate the percentage of clicks for each topic
    topic_click_counts = content_with_topics.groupby(['Topic', 'clicked']).Document.count().unstack(fill_value=0)
    topic_click_counts['pct_clicked'] = topic_click_counts[True] / (topic_click_counts[True] + topic_click_counts[False])
    sorted_topics = topic_click_counts.sort_values(by='pct_clicked', ascending=False).reset_index()
    topics = list(sorted_topics["Topic"])

    llm_pipeline = load(LLM_MODEL_NAME)
    new_topics = set()
    topic_i = 0
    while len(new_topics) < num_topics and topic_i < len(topics):
        topic = topics[topic_i]
        topic_df = content_with_topics.loc[content_with_topics['Topic'] == topic]
        topic_df = topic_df.sort_values(by="last_modified_timestamp", ascending=False)
        topic_df = topic_df.iloc[:10]
        print(f"Creating new topic from {topic_df['Name'].iloc[0]} with {len(topic_df)} articles.")
        new_topic = create_topic_from_previous_topic(topic_df=topic_df, llm_pipeline=llm_pipeline)
        if new_topic not in exa_content_generators_topics:
            new_topics.add(new_topic)
        else:
            print("Created replicate topic:", new_topic)
        topic_i += 1

    return new_topics
    